# -*- coding: utf-8 -*-
"""redline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qIgDtNxI_jIw4iMilwF_vu3seFIqkdOP
"""

# ========================
# 0. Install dependencies (run only in Colab)
# ========================
!pip install -q transformers datasets scikit-learn matplotlib seaborn

# ========================
# 1. Imports
# ========================
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import torch
from google.colab import files, drive

from datasets import Dataset, DatasetDict
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

# ========================
# 2. Mount Google Drive
# ========================
drive.mount('/content/drive')
drive_model_path = "/content/drive/MyDrive/fine_tuned_indicbert"  # Change folder path if needed

# ========================
# 3. Upload CSVs
# ========================
print("Upload train_1mg.csv")
uploaded_train_1mg = files.upload()
train_1mg = pd.read_csv(list(uploaded_train_1mg.keys())[0])

print("Upload test_1mg.csv")
uploaded_test_1mg = files.upload()
test_1mg = pd.read_csv(list(uploaded_test_1mg.keys())[0])

print("Upload train_webmd.csv")
uploaded_train_webmd = files.upload()
train_webmd = pd.read_csv(list(uploaded_train_webmd.keys())[0])

print("Upload test_webmd.csv")
uploaded_test_webmd = files.upload()
test_webmd = pd.read_csv(list(uploaded_test_webmd.keys())[0])

# ========================
# 4. Rename columns
# ========================
for df in [train_1mg, test_1mg, train_webmd, test_webmd]:
    df.rename(columns={"question_english": "text", "Manual_Intent": "label"}, inplace=True)

# Merge datasets
train_df = pd.concat([train_1mg, train_webmd], ignore_index=True)
test_df = pd.concat([test_1mg, test_webmd], ignore_index=True)

# ========================
# 5. Visualize Intent Distribution
# ========================
plt.figure(figsize=(10, 6))
sns.countplot(y="label", data=train_df, order=train_df["label"].value_counts().index, palette="viridis")
plt.title("Intent Distribution in Training Data")
plt.xlabel("Count")
plt.ylabel("Intent")
plt.show()

# ========================
# 6. Encode Labels
# ========================
labels = sorted(train_df["label"].unique())
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for label, i in label2id.items()}

train_df["label"] = train_df["label"].map(label2id)
test_df["label"] = test_df["label"].map(label2id)

# ========================
# 7. HuggingFace Dataset
# ========================
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)
dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

# ========================
# 8. Tokenizer & Model
# ========================
model_name = "ai4bharat/indic-bert"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id
)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print("Using device:", device)

# ========================
# 9. Metrics Function
# ========================
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# ========================
# 10. Training Arguments
# ========================
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_strategy="steps",
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="none"
)

# ========================
# 11. Trainer
# ========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# ========================
# 12. Train
# ========================
trainer.train()

# ========================
# 13. Evaluate
# ========================
results = trainer.evaluate()
print("Final Evaluation:", results)

# ========================
# 14. Confusion Matrix
# ========================
preds = trainer.predict(tokenized_datasets["test"])
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=1)

cm = confusion_matrix(y_true, y_pred, labels=list(id2label.keys()))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(id2label.values()))
disp.plot(cmap="Blues", xticks_rotation=45)
plt.title("Confusion Matrix - IndicBERT Fine-tuned")
plt.show()

# ========================
# 15. Training Curves
# ========================
logs = trainer.state.log_history

train_loss = [x["loss"] for x in logs if "loss" in x]
eval_loss = [x["eval_loss"] for x in logs if "eval_loss" in x]
min_len = min(len(train_loss), len(eval_loss))
train_loss = train_loss[:min_len]
eval_loss = eval_loss[:min_len]
epochs = range(1, min_len + 1)

plt.plot(epochs, train_loss, label="Training Loss")
plt.plot(epochs, eval_loss, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.show()

# ========================
# 16. Save Fine-tuned Model to Google Drive
# ========================
trainer.save_model(drive_model_path)
tokenizer.save_pretrained(drive_model_path)
print(f"Model and tokenizer saved to: {drive_model_path}")

# ========================
# 17. Load Fine-tuned Model Later (example)
# ========================
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# model = AutoModelForSequenceClassification.from_pretrained(drive_model_path)
# tokenizer = AutoTokenizer.from_pretrained(drive_model_path)